# Recurrent Neural Network Regularization by Zaremba et al. (2014)
This repository contains the replication of "Recurrent Neural Network Regularization" by Zaremba et al. (2014).

It is one of the earliest successful applications of Dropout on RNNs and had achieved state of the art results on word-level language modeling task on Penn Treebank dataset back in its day.

The paper can be found at: [https://arxiv.org/abs/1409.2329](https://arxiv.org/abs/1409.2329)  
While the original code written in Lua and Torch can be found at: [https://github.com/wojzaremba/lstm](https://github.com/wojzaremba/lstm)

I have replicated the paper using Python 3.7 and PyTorch 1.2 with CUDA Toolkit 10.0. 
